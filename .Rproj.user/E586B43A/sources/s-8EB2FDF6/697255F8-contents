# One-Way Repeated Measures ANOVA {#Repeated}

 [Screencasted Lecture Link](https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=c8f5737f-d00d-4fa4-ba3c-ad8b01762258) 
 
```{r  include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
options(scipen=999)#eliminates scientific notation
```

In the prior lessons, a critical assumption is that the observations must be "independent."  That is, related people (partners, parent/child, manager/employee) cannot comprise the data and there cannot be multiple waves of data for the same person. Repeated measures ANOVA is created specifically for this *dependent* purpose. This lessons focuses on the one-way repeated measures ANOVA, where we measure changes across time.

## Navigating this Lesson

There is just over one hour of lecture.  If you work through the materials with me it would be plan for an additional two hours

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReCenterPsychStats) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Learning objectives from this lecture include the following:

* Evaluate the suitability of a research design/question and dataset for conducting a one-way repeated measures ANOVA; identify alternatives if the data is not suitable.
* Hand-calculate a one-way repeated measures NOVA
  - describing the partitioning of variance as it relates to model/residual; within/between
* Test the assumptions for one-way repeated measures ANOVA.
* Conduct a one-way repeated measures ANOVA (omnibus and follow-up) in R.
* Interpret output from the one-way repeated measures ANOVA (and follow-up). 
* Prepare an APA style results section of the one-way repeated measures ANOVA output.
* Demonstrate how an increased sample size increases the power of a statistical test.

### Planning for Practice

The suggestions for homework are graded in complexity with more complete descriptions at the end of the chapter follow these suggestions.

* Rework the problem in the chapter by changing the random seed in the code that simulates the data.  This should provide minor changes to the data, but the results will likely be very similar. 
* There were no additional variables in this example. However, you'll see we do have an issue with statistical power. Perhaps change the sample size to see if it changes (maybe stabilizes?) the results.
* Conduct a one-way repeated measures ANOVA with data to which you have access. This could include data you simulate on your own or from a published article.

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

* Repeated Measures ANOVA in R: The Ultimate Guide. (n.d.). Datanovia. Retrieved October 19, 2020, from https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r 
  - This website is an excellent guide for both one-way repeated measures and mixed design ANOVA. A great resource for both the conceptual and procedural.  This is the guide I have used for the basis of the lecture.  Working through their example would be great additional practice.
* Green, S. B., & Salkind, N. J. (2014). One-Way Repeated Measures Analysis of Variance (Lesson 29). In Using SPSS for Windows and Macintosh: Analyzing and understanding data (Seventh edition., pp. 209–217). Pearson.
  - For years I taught from the Green and Salkind text. Even though it was written for SPSS, the authors do a terrific job of walking the reader through the one-way repeated measures logic and process.
* Amodeo, A. L., Picariello, S., Valerio, P., & Scandurra, C. (2018). Empowering transgender youths: Promoting resilience through a group training program. Journal of Gay & Lesbian Mental Health, 22(1), 3–19.
  - This mixed methods (qualitative and quantitative) includes a one-way repeated measures example.  While the entire article is a good read, if you are short on time, focus your reading on the portions that address relevant to the use of the one-way repeated measures ANOVA:  research design, procedure, results.

### Packages

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r  }
#will install the package if not already installed
#if(!require(tidyverse)){install.packages("tidyverse")} #manipulate data
#if(!require(psych)){install.packages("psych")} 
#if(!require(ggpubr)){install.packages("ggpubr")} #easy plots
#if(!require(rstatix)){install.packages("rstatix")} #pipe-friendly R functions
#if(!require(data.table)){install.packages("data.table")} #pipe-friendly R functions
#if(!require(reshape2)){install.packages("reshape2")} #pipe-friendly R functions
#if(!require(WebPower)){install.packages("WebPower")} #power analysis tools for repeated measures
#if(!require(MASS)){install.packages("MASS")} #power analysis tools for repeated measures
```

## Introducing One-way Repeated Measures ANOVA

There are a couple of typical use cases for one-way repeated measures ANOVA.  In the first, the research participant is assessed in multiple conditions -- with no interested in change-over-time.

In the Green and Salkind [-@green_using_2014] statistics text, the one-way repeated measures example used in the lesson compared a teachers' perception of stress when responding to parents, teachers, and school administrators.

![Illustration of a research design appropriate for one-way repeated measures ANOVA](images/oneway_repeated/repeated_conditions.jpg)

Another common use case is about time, with the classic design being a pre-test, an intervention, a post-test and a follow up.  In this we likely hope that there is a positive change from pre-to-post and that that change either stays constant (from post-to-follow-up) or, perhaps, increases even further.  Our research vignette is interested in change-over-time.

![Illustration of a research design appropriate for one-way repeated measures ANOVA](images/oneway_repeated/repeated_design.jpg){#id .class width=1000 height=100px}

### Workflow for Oneway Repeated Measures ANOVA

The following is a proposed workflow for conducting a one-way repeated measures ANOVA. 

![Image of a workflow for the one-way repeated measures ANOVA](images/oneway_repeated/wf_repeated.jpg)

Steps involved in analyzing the data include:

1. Prepare (upload) data.
2. Explore data
     + graphs
     + descriptive statistics
3. Checking distributional assumptions
     + assessing normality via skew, kurtosis, Shapiro Wilks
     + checking or violation of the *sphericity* assumption with Mauchly's test; if violated interpret the corrected output or use a multivariate approach for the analysis
4. Compute the omnibus ANOVA 
5. Compute post-hoc comparisons, planned contrasts, or polynomial trends
6. Managing Type I error
7. Sample size/power analysis (which you should think about first -- but in the context of teaching ANOVA, it's more pedagogically sensible, here)

## Research Vignette

Amodeo [@amodeo_empowering_2018] and colleagues conducted a mixed methods study (qualitative and quantitative) to evaluate the effectiveness of an empowerment, peer-group-based, intervention with participants (*N* = 8) who experienced transphobic episodes. Focus groups used qualitative methods to summarize emergent themes from the program (identity affirmation, self-acceptance, group as support) and a one-way, repeated measures ANOVA provided evidence of increased resilience from pre to three-month followup.

Eight participants (seven transgender women and one genderqueer person) participated in the intervention.  The mean age was 28.5 (*SD* = 5.85).  All participants were Italian.

The within-subjects condition was wave:

* T1, beginning of training
* Training, three 8-hour days, 
  - content included identity and heterosexism, sociopolitical issues and minority stress, resilience and empowerment
* T2, at the conclusion of the 3-day training
* Follow-up session 3 months later
* T3, at the conclusion of the +3 month follow-up session

The dependent variable (assessed at each wave) was a 14-item resilience scale [@wagnild_development_1993]. Items were assessed on a 7-point scale ranting from *Strongly disagree* to *Strongly agree* with higher scores indicating higher levels of resilience.  An example items was, "I usually manage one way or another."

![Diagram of the research design for the Amodeo et al study](images/oneway_repeated/Amodio_design.jpg){#id .class width=1000 height=100px}

### Code for simulating the data used today.

Below is the code I used to simulate data. The following code assumes 8 participants who each participated in 3 waves (pre, post, followup).
```{r }
set.seed(2022)
ID<-factor(c(rep(seq(1,8),each=3)))#gives me 8 numbers, assigning each number 3 consecutive spots, in sequence
Resilience<-rnorm(24,mean=c(5.7,6.21,6.26),sd=c(.88,.79,.37)) #gives me a column of 24 numbers with the specified Ms and SD
Wave<-rep(c("Pre","Post", "FollowUp"),each=1,8) #repeats pre, post, follow-up once each, 8 times
Amodeo_long<-data.frame(ID, Wave, Resilience)
#OneWay_mod<-aov(Resilience~Wave + Error(ID/(Wave)), AmodeoSIM)
#summary(OneWay_mod)
#model.tables(OneWay_mod,"means")
```

Let's take a look at the structure of our variables.  We want ID to be a factor, Resilience to be numeric, and Wave to be an ordered factor (Pre, Post, FollowUp).

```{r}
str(Amodeo_long)
```
We just need to change Wave to be an ordered factor.

```{r}
Amodeo_long$Wave <- factor(Amodeo_long$Wave, levels = c("Pre", "Post", "FollowUp"))
```

And we check the structure again.
```{r}
str(Amodeo_long)
```

**Shape Shifters**

The form of our data matters.  The simulation created a *long* form (formally called the *person-period* form) of data.  That is, each observation for each person is listed in its own row. In this dataset where we have 8 people with 3 observation (pre, post, follow-up) each, we have 24 rows. This is convenient, because this is the form we need for repeated measures ANOVA.  

However, for some of the calculations (particularly those we will do by hand), we need the data to be in its typical wide form (formally called the *person level* form). We can do this with the *data.table* and *reshape2*()* packages.  

```{r }
library(reshape2)
# Create a new df (Amodeo_wide)
# Identify the original df
# We are telling it to connect the values of the Resilience variable its respective Wave designation
Amodeo_wide <- reshape2::dcast(data = Amodeo_long, formula =ID~Wave, value.var = "Resilience")
#doublecheck to see if they did what you think
str(Amodeo_wide)
Amodeo_wide$ID <- factor(Amodeo_wide$ID)

```
In this reshape script, I asked for a quick structure check.  The format of the variables looks good.

If you want to export these data as files to your computer, remove the hashtags to save (and re-import) them as .csv ("Excel lite") or .rds (R object) files. This is not a necessary step.  

The code for the .rds file will retain the formatting of the variables, but is not easy to view outside of R.  I would choose this option.
```{r}
#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with
#saveRDS(Amodeo_long, "Amodeo_longRDS.rds")
#saveRDS(Amodeo_wide, "Amodeo_wideRDS.rds")
#bring back the simulated dat from an .rds file
#Amodeo_long <- readRDS("Amodeo_longRDS.rds")
#Amodeo_wide <- readRDS("Amodeo_wideRDS.rds")
```

An option is to write them as .csv files. The code for .csv will likely lose any variable formatting, but they is easy to view and manipulate in Excel. If you choose this option, you will probably need to re-run the code to reformat the Wave variable as an ordered factor
```{r}
#write the simulated data  as a .csv
#write.table(Amodeo_long, file="Amodeo_longCSV.csv", sep=",", col.names=TRUE, row.names=FALSE)
#write.table(Amodeo_wide, file="Amodeo_wideCSV.csv", sep=",", col.names=TRUE, row.names=FALSE)
#bring back the simulated dat from a .csv file
#Amodeo_long <- read.csv ("Amodeo_longCSV.csv", header = TRUE)
#Amodeo_wide <- read.csv ("Amodeo_wideCSV.csv", header = TRUE)
```

### Quick peek at the data

Before we get into the statistic let's look at our data.
```{r}
str(Amodeo_long)
```
In the following output, see the order of presentation on the first screen of output.  Even though we have ordered our factor so that "Pre" is first, the *describeBy()* function seems to be ordering them alphabetically.

```{r}
library(psych)
psych::describeBy(Amodeo_long$Resilience, Wave, mat = TRUE, data = Amodeo_long)
```

Another view (if we use the wide file)

```{r}
psych::describe(Amodeo_wide)
```
Our means suggest that resilience increases from pre to post, then declines a bit. We use one-way repeated measures ANOVA to learn if there are statistically significant differences between the pairs of means and over time.

Let's also take a quick look at a boxplot of our data.  

```{r }
boxplot (Resilience ~ Wave, data = Amodeo_long, xlab = "Wave", ylab = "Resilience", n.label = TRUE)
```


## Working the One-Way Repeated Measures ANOVA (by hand)

Before working our problem in R, let's gain a conceptual understanding by partitioning the variance by hand.

In repeated measures ANOVA:  $SS_T = SS_B + SS_W$, where

*  B = between-subjects variance
*  W = within-subjects variance
   - $SS_W = SS_M + SS_R$

What differs is that $SS_M$ and $SS_R$ (model and residual, are found in $SS_W$)

*  $SS_T = SS_B + (SS_M + SS_R)$

![Demonstration of partitioning variance](images/oneway_repeated/SourceTable.jpg){#id .class width=500 height=250px}

### Sums of Squares Total

$$SS_{T}= \sum (x_{i}-\bar{x}_{grand})^{2}$$
$$SS_{T}= s_{grand}^{2}(n-1)$$
Degrees of freedom for $SS_T$ is *N* - 1, where *N* is the total number of cells.

Let's take a moment to *hand-calculate* $SS_{T}$ (but in R).

Our grand (i.e., overall) mean is 
```{r }
mean(Amodeo_long$Resilience)
```

Subtracting the grand mean from each resilience score yields a mean difference.
```{r }
library(tidyverse)

Amodeo_long <- Amodeo_long %>% 
  mutate(m_dev = Resilience-mean(Resilience))
```
Pop quiz:  What's the sum of our new *m_dev* variable?

```{r }
sum(Amodeo_long$m_dev)
```

If we square those mean deviations:
```{r }
Amodeo_long <- Amodeo_long %>% 
  mutate(m_devSQ = m_dev^2)
```

If we sum the squared mean deviations:
```{r }
sum(Amodeo_long$m_devSQ)
```
This value, the sum of squared deviations around the grand mean, is our $SS_T$; the associated *degrees of freedom* is 23 (24 - 1; *N* - 1).

### Sums of Squares Within for Repated Measures ANOVA

$$SS_W = s_{person1}^{2}(n_{1}-1)+s_{person2}^{2}(n_{2}-1)+s_{person3}^{2}(n_{3}-1)+...+s_{personk}^{2}(n_{k}-1)$$
Degrees of freedom (df) within is *N - k*; or the summation of the df for each of the persons.

```{r }
library(psych)
describeBy(Resilience ~ ID, data = Amodeo_long, mat = TRUE, digits = 3)
```
With 8 people, there will be 8 chunks of the analysis, in each:

* SD squared (to get the variance)
* multiplied by #observations less 1

```{r }
(.605^2*(3-1)) + (.760^2*(3-1)) + (.992^2*(3-1))+ (.568^2*(3-1))+ (.824^2*(3-1))+ (.146^2*(3-1))+ (.248^2*(3-1)) + (.553^2*(3-1))
```

### Sums of Squares Model -- Effect of Time

$$SS_{M}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}$$
Degrees of freedom will be k - 1 (number of levels, minus one).

```{r }
library(psych)
psych::describe(Amodeo_wide)
```
In this case, we are interested in change in resilience over time.  Hence, *time* is our mode.  In our equation, we have three chunks representing the pre, post, and follow-up *conditions* (waves). From each, we subtract the grand mean, square it, and multiply by the *n* observed in each wave.

Degrees of freedom (df) for $SS_M$ is *k* - 1

Let's calculate grand mean; that is the resilience score for all participants across all waves.
```{r}
mean(Amodeo_long$Resilience)
```

Now we can calculate the $SS_M$.
```{r }
(8*(6.14 - 6.017)^2) + (8*(6.33 - 6.017)^2) + (8*(5.59 - 6.017)^2) 
#df is 3-1 = 2
```

### Sums of Squares Residual

Let's take the easy way out, given that $SS_W = SS_M + SS_R$:

$SS_w$ = 6.636
$SS_M$ = 2.363
```{r }
6.636 - 2.363
```
Degrees of freedom (also taking the easy way out) is calculated by subtracting the $SS_M$  from $SS_W$.
```{r }
16-2
```

### Sums of Squares Between

Not used in our calculations today, but also calculated easily.  Given that $SS_T$ = $SS_W$ + $SS_B$:

$SS_T$ = 11.66; *df* = 23
$SS_W$ = 6.64; *df* = 16
```{r  }
11.66 - 6.64
23-16
```
$SS_B$ = 5.02, *df* = 7


![Screenshot of the ANOVA source Table](images/oneway_repeated/SourceTable.jpg)
Looking again at our sourcetable, we can move through the steps to calculate our *F* statistic.

### Mean Squares Model & Residual

Now that we have the Sums of Squares, we can calculate the mean squares (we need these for our $F$ statistic).

$$MS_M = \frac{SS_{M}}{df^{_{M}}}$$
```{r }
# mean squares for the model
2.36/2
```

And $MS_R=$
$$MS_R = \frac{SS_{R}}{df^{_{R}}}$$
Recall, df for the residual is N - k
(in our case that's 90 - 3)

```{r }
# mean squares for the residual
4.27 / 14
```

### *F* ratio

$$F = \frac{MS_{M}}{MS_{R}}$$
```{r }
1.18 / .305
```

To find the $F_{CV}$ we can use an [F distribution table](https://www.statology.org/f-distribution-table/).

Or use a look-up function, which follows this general form:  qf(p, df1, df2. lower.tail=FALSE)
```{r}
#looking up the F critical values
qf(.05, 2, 14, lower.tail=FALSE)#Model F critical value
```

Our example has 2 (numerator) and 14 (denominator) degrees of freedom.  Rolling down to the table where $\alpha  = .05$, we can see that any $F$ value > 3.73 will be statistically significant.  Our $F$ = 3.87, so we have (just barely) exceeded the thresshhold.  This is our *omnibus F*.  We know there is at least 1 statistically significant difference between our pre, post, and follow-up conditions.

## Working the One-Way ANOVA with R packages

### Testing the assumptions
Well start by testing the assumptions. Here is our place in the one-way ANOVA decision tree:

![Image of our position in the workflow for the one-way repeated measures ANOVA](images/oneway_repeated/wf_rptd_assumptions.jpg)

There are several different ways to conduct a repeated measures ANOVA.  Each has different assumptions/requirements.  These include:

* univariate statistics
  - this is what we will use today
* multivariate statistics
  - same as univariate, except it does not require the sphericity assumption
* multi-level modeling/hierarchical linear modeling
  - a different statistic altogether; stay tuned

#### Univariate assumptions for repeated measures ANOVA

* The cases represent a random sample from the population, and there is no dependency in the scores between participants. 
* There are no significant outliers in any cell of the design
  - Check by visualizing the data using box plots and the function *identify_outliers()* [rstatix]
  - Conduct a Shapiro-Wilk test of normality for each of the levels of the DV
  - Visually examine qq plots
* The dependent variable is normally distributed in the population for each level of the within-subjects factor.
* The population variance of difference scores computed between any two levels of a within-subjects factor is the same value regardless of which two levels are chosen; termed the **sphericity assumption**. 
  - Akin to compound symmetry (both variances across conditions are equal).
  - Akin to the homogeneity of variance assumption in between-group designs. 
  - Sometimes called the homogeneity-of-variance-of-differences assumption. 
  - Statistically evaluated with *Mauchly's test.* If Mauchly's *p* < .05, there are statistically significant differences.  The *anova_test()* function in the *rstatix* package reports Mauchly's and two alternatives to the traditional *F* that correct the values by the degree to which the sphericity assumption is violated. 

Demonstrating sphericity:

Using the data from our motivating example, I calculated differences for each of the time variables.  When we get into the analysis, we will use *Mauchly's test* in hopes that there are non-significant differences in variances between all three of the comparisons.

We are only concerned with the sphericity assumption if there are three or more groups.

$$variance_{A-B} \approx variance_{A-C}\approx variance_{B-C}$$

![Demonstration of unequal variances](images/oneway_repeated/mauchly.jpg){#id .class width=500 height=250px}

#### Any outliers?

The boxplot is one common way for identifying outliers.  The boxplot uses the median and the lower (25th percentile) and upper (75th percentile) quartiles.  The difference bewteen Q3 and Q1 is the *interquartile range* (IQR).  Outliers are generally identified when values fall outside these lower and upper boundaries:

* Q1 - 1.5xIQR
* Q3 + 1.5xIQR

Extreme values occur when values fall outside these boundaries:

* Q1 - 3xIQR
* Q3 + 3xIQR

Let's take a look at a boxplot.
```{r }
library(ggpubr)
bxp <- ggboxplot(Amodeo_long, x = "Wave", y = "Resilience", add = "point", xlab = "Assessment Wave", ylab = "Self-Perception of Resilience")
bxp
```
The package *rstatix* has features that allow us to identify outliers.
```{r }
library(rstatix)
Amodeo_long %>%
  group_by(Wave)%>%
  identify_outliers(Resilience)

#?identify_outliers
```

No outliers are identified. Visual inspection of boxplots, assisted by the *identify_outliers()* function in the *rstatix* package (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated no outliers.

#### Assessing normality 

```{r }
library(psych)
psych::describeBy(Resilience ~ Wave, mat=TRUE, data = Amodeo_long)
```

Our skew and kurtosis values fall below the thresshholds of concern [@kline_principles_2016]:

* < 3 for skew
* 8 - 20 indicates extreme skew for kurtosis


We can use the Shapiro-Wilk test for a formal detection of normality. When *p* < .05, it indicates that the distribution is statistically significantly different than a normal one.  Therefore, *p* > .05 indicates we did not violate the normal distribution assumption.

```{r }
Amodeo_long %>%
  group_by(Wave) %>%
  shapiro_test(Resilience)
```

Great!  The *p* value is > .05 for each of the cells.

The Shapiro-Wilk test is sensitive to sample size [@noauthor_repeated_nodate].  Samples > 50 may lead to *p* values that are < .05, even when non-normality is not problematic.  Therefore a second check with a QQ plot can be helpful.

```{r }
ggqqplot(Amodeo_long, "Resilience", facet.by = "Wave")
```


We already have a non-significant *p* from the Shapiro-Wilk; and these dots stay pretty well on the prediction line.

**APA Assumption Write-up So Far**

Repeated measures ANOVA has several assumptions regarding outliers, normality, and sphericity. Visual inspection of boxplots for each wave of the design, assisted by the *identify_outliers()* function in the *rstatix* package (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated no outliers. Regarding normality, no values of skew and kurtosis (at each wave of assessment) fell within cautionary ranges for skew and kurtosis [@kline_principles_2016]. Additionally, the Shapiro-Wilk tests applied at each wave of the design were non-significant.

#### Assumption of Sphericity

The sphericity assumption is automatically checked with Mauchley's test during the computation of the ANOVA when the *anova_test()* [rstatix package] is used. When the *get_anova_table()* [rstatix] function is used, the Greenhouse-Geisser sphericity correction is automatically applied to factors violating the sphericity assumption.

The effect size, $\eta^2$ is reported in the ges column.

### Omnibus ANOVA

```{r }
str(Amodeo_long)
```

```{r }
library(rstatix)
RM_AOV <- anova_test(data = Amodeo_long, dv = Resilience, wid = ID, within = Wave)
RM_AOV
```
From the ANOVA object:  $F(2,14) = 3.91, p = 0.045, \eta^2 = 0.203$

From the Mauchly's Test for Sphericity object we learn that we did not violate the sphericity assumption:  $W = 0.566, p = .182$

From the Sphericity Corrections object are output for two alternative corrections to the *F* statistic, the Greenhouse-Geiser epsilon (GGe) and Huynh-Feldt epsilon (HFe).  Because we did not violate the sphericity assumption we do not need to use it. Notice that these two tests adjust our df (both numerator and denominator) to have a more conservative p value.

If we needed to write it up, the *F* string might look like this:

The Greenhouse Geiser estimate was 0.698 the correct omnibus was *F*(1.4, 9.77) = 3.91, *p* = .068.
The Huyhn Feldt estimate was 0.817 and the corrected omnibus was *F* (1.63, 11.44) = 3.91 *p* = .057.

You might be surprised that we are at follow-up already.  The test of the sphericity assumption occured at the same time we evaluated the omnibus ANOVA.

![Image of our position in the workflow for the one-way repeated measures ANOVA](images/oneway_repeated/wf_rptd_omnibus.jpg)

### Follow-up

Note that when I am calculating these pairwise *t* tests, I am creating an object (named "pwc").  The object will be a helpful tool in creating a Figure and an APA Style table.

```{r }
pwc <- Amodeo_long %>%
  pairwise_t_test(Resilience~Wave, paired = TRUE, p.adjust.method = "bonferroni")
pwc
```

So why didn't we get significance in the follow-up?  

* Our omnibus *F* was right at the margins
  - a larger sample size (assuming that the effects would hold) would be really useful.
  - there could be significance if we compared pre to the combined effects of post and follow-up.
  
What about Type I error?  With only three possible post-omnibus comparisons, I will claim the Tukey LSD approach and not adjust the alpha to a more conservative level [@green_using_2014].

Coolest boxplots ever
```{r }
pwc <- pwc %>% add_xy_position(x = "Wave")
bxp + 
  stat_pvalue_manual(pwc) +
  labs(
    subtitle = get_test_label(RM_AOV, detailed = TRUE),
    caption = get_pwc_label(pwc)
  )
```
Unfortunately, the *apaTables* package does not work with the *rstatix* package, so a table would need to be crafted by hand.

### Results Section

Repeated measures ANOVA has several assumptions regarding outliers, normality, and sphericity. Visual inspection of boxplots for each wave of the design, assisted by the *identify_outliers()* function in the *rstatix* package (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated no outliers. Regarding normality, no values of skew and kurtosis (at each wave of assessment) fell within cautionary ranges for skew and kurtosis [@kline_principles_2016]. Additionally, the Shapiro-Wilk tests applied at each wave of the design were non-significant. A non-significant Mauchley's test ($W = 0.566, p = .182$) indicated that the sphericity assumption was not violated. 

The omnibus ANOVA was significant: $F(2,14) = 3.91, p = 0.045, \eta^2 = 0.203$. We followed up with all pairwise comparisons.  Curiously, and in spite of a significant omnibus test, there were not statistically significant differences between any of the pairs.  Regarding pre versus post, *t* = -2.15, *p*= .069.  Regarding pre versus follow-up, *t* = -2.00, *p* = .068.  Regarding post versus follow-up, *t* = 1.059, *p*= .325. Because there were only three pairwise comparisons subsequent to the omnibus test, alpha was retained at .05 [@green_using_2014].  While the trajectories from pre-to-post and pre-to-follow-up were in the expected direction, the small sample size likely contributed to a Type II error.  Descriptive statistics are reported in Table 1 and the differences are illustrated in Figure 1.

#### Creating an APA Style Table**

While I have not located a package that will take *rstatix* output to make an APA style table, we can use the *MASS* package to write the pwc object to a .csv file, then manually make our own table.

```{r }
library(MASS)
write.matrix(pwc, sep = ",", file = "PWC.csv")
```


**Let's compare this to the write-up in the Amodeo et al.[-@amodeo_empowering_2018]article.**

* The *F* string is presented in the Table 1 note (*F*[1.612, 11.283]) = 6.390, *p* = 0.18, $\eta _{p}^{2}$)
  - we can tell from the df that the *p* value has been had a correction for violation of the sphericity assumption
* Table 1 also reports all of the post-hoc, pairwise comparisons.  There is no mention of control for Type I error.  Had they used a traditional Bonferroni, they would have needed to reduce the alpha to (k*(k-1)/2) and then divide .05 by that number.

```{r }
(3 * (3-1))/2
.05/3
```
Although they report 6 comparisons; 3 are repeated because they are merely in reverse.  Yet, the revised alpha would be .016 and the one, lone, comparison would not have been statistically significant.  That said, we can invoke the Tukey LSD because there are only 3 comparisons and holding alpha at .05 can be defended [@green_using_2014].
* Regarding the presentation of the results
  - there is no figure
  - there is no data presented in the text; all data is presented in Table 1
* Regarding the research design and its limitations
  - the authors note that a control condition would have better supported the conclusions
  - the authors note the limited sample size and argue that this is a difficult group to recruit for intervention and evaluation
  - the article is centered around the qualitative aspect of the design; the quantitative portion is secondary

![Another peek at the research design for the Amodeo et al study](images/oneway_repeated/Amodio_design.jpg){#id .class width=1000 height=100px}

## Power in Repeated Measures ANOVA

The package *wp.rmanova* was designed for power analysis in repeated measures ANOVA.

Power analysis allows us to determine the probability of detecting an effect of a given size with a given level of confidence. Especially when we don't achieve significance, we may want to stop. 

In the *WebPower* package, we specify 6 of 7 interrelated elements; the package computes the missing one.

n = sample size (number of individuals in the whole study)
ng = number of groups
nm = number of measurements/conditions/waves
f = Cohen's *f* (an effect size; we can use a conversion calculator)
nscor = the Greenhouse Geiser correction from our ouput; 1.0 means no correction was needed and is the package's default; < 1 means some correction was applied. 
alpha = is the probability of Type I error; we traditionally set this at .05 
power = 1 - P(Type II error) we traditionally set this at .80 (so anything less is less than what we want)
type = 0 is for between-subjects, 1 is for repeated measures, 2 is for interaction effect. 

I used *effectsize* packages converter to transform our $\eta^2$ to Cohen's *f*.

```{r}
library(effectsize)
eta2_to_f(.203) 
```

```{r }
library(WebPower)
wp.rmanova(n=8, ng=1, nm=3, f = .5047, nscor = .689, alpha = .05, power = NULL, type = 1)
```

In reverse, setting *power* at .80 (the traditional value) and changing *n* to *NULL* yields a recommended sample size.    

In many cases we won't know some of the values in advance.  We can make best guesses based on our review of the literature.  In the script below:

* nscor is the degree of violation of the sphericity assumption.  If we think we won't violate it, we can enter 1.0 or leave it out (the wp.rmanova default is 1.0)
* f is the effect size estimate; Cohen suggests that f values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes, respectively. 

```{r }
wp.rmanova(n=NULL, ng=1, nm=3, f = .5047, nscor = .689, alpha = .05, power = .80, type = 1)
```

## Practice Problems

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. In any case, you will be expected to:

* test the statistical assumptions
* conduct a one-way, including
  - omnibus test and effect size
  - conduct follow-up testing 
* write a results section to include a figure and tables

### Problem #2: Change the Random Seed

If repeated measures ANOVA is new to you, perhaps change the random seed and follow-along with the lesson.  

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data |      5            |_____  |           
|2. Evaluate statistical assumptions     |      5            |_____  |
|3. Conduct omnibus ANOVA (w effect size)|      5           | _____  |  
|4. Conduct all possible pairwise comparisons (like in the lecture)| 5 |_____  |               
|5. Describe approach for managing Type I error|    5        |_____  |   
|6. APA style results with table(s) and figure|    5        |_____  |       
|7. Explanation to grader                 |      5        |_____  |
|**Totals**                               |      35       |_____  |          


### Problem #2: Increase *N*

Our analysis of the Amodio et al. [-@amodio_empowering_2018] data failed to find significant increases in resilience from pre-to-post through follow-up.  Our power analysis suggested that a sample size of 50 would be sufficient to garner statistical significance. The new dataset has been resimulated with this new sample size.  

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data |      5            |_____  |           
|2. Evaluate statistical assumptions     |      5            |_____  |
|3. Conduct omnibus ANOVA (w effect size)|      5           | _____  |  
|4. Conduct all possible pairwise comparisons (like in the lecture)| 5 |_____  |               
|5. Describe approach for managing Type I error|    5        |_____  |   
|6. APA style results with table(s) and figure|    5        |_____  |       
|7. Explanation to grader                 |      5        |_____  |
|**Totals**                               |      35       |_____  |          


### Problem #2: Try Something Entirely New

Using data for which you have permission and access (e.g.,  IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a one-way repeated measures ANOVA. Please have at least 3 levels for the predictor variable. 

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data |      5            |_____  |           
|2. Evaluate statistical assumptions     |      5            |_____  |
|3. Conduct omnibus ANOVA (w effect size)|      5           | _____  |  
|4. Conduct all possible pairwise comparisons (like in the lecture)| 5 |_____  |               
|5. Describe approach for managing Type I error|    5        |_____  |   
|6. APA style results with table(s) and figure|    5        |_____  |       
|7. Explanation to grader                 |      5        |_____  |
|**Totals**                               |      35       |_____  |           


## Bonus Reel: 

![Image of a filmstrip](images/film-strip-1.jpg){#id .class width=620 height=211}


Without the *rstatix* helper package, here is how the analysis would be run in the package, *car.*  Note that this package results in the multivariate output.  The *p* value of the omnibus *F* was non-significant from the start (*p* = .213).


```{r }
library(car)

waveLevels <- c(1,2,3)
waveFactor <- as.factor(waveLevels)
waveFrame <- data.frame(waveFactor)
waveBind <-cbind(Amodeo_wide$Pre, Amodeo_wide$Post, Amodeo_wide$FollowUp)
waveModel<- lm(waveBind~1)
analysis <-Anova(waveModel, idata=waveFrame, idesign=~waveFactor)
summary(analysis)
```

```{r include=FALSE}
sessionInfo()
```


